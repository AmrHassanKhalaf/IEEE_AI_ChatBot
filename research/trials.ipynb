{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389bf50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\amrha\\\\Downloads\\\\IEEE_AI_ChatBot\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcb4b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05e5ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2337feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents=loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62cb32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ccad727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b64af",
   "metadata": {},
   "source": [
    "## preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35c447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "def filter_to_minimal_docs(docs:List[Document]) -> List[Document]:\n",
    "    \"\"\"Given a list of Documents objects, return a new list of Document objects containing only 'source' in metadata and the original page contant.\"\"\"\n",
    "    minimal_docs :List[Document] = []\n",
    "\n",
    "    for doc in docs:\n",
    "        src= doc.metadata.get('source')\n",
    "        minimal_docs.append(Document(page_content=doc.page_content, metadata={'source': src}))\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51acf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_docs = filter_to_minimal_docs(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a39883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# spitting the documents into smaller chunks\n",
    "def text_splitter(minimal_docs):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    texts_chunk=text_splitter.split_documents(minimal_docs)\n",
    "    return texts_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aa5eca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "texts_chunk = text_splitter(minimal_docs)\n",
    "print(len(texts_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d280858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amrha\\AppData\\Local\\Temp\\ipykernel_54608\\30168171.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def download_embeddings():\n",
    "    \"\"\"\n",
    "    Download and return the HuggingFace embeddings model.\n",
    "    \"\"\"\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6e405a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c8b609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "vector=embedding.embed_query(\"Hello world\")\n",
    "print(len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a57a2212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amrha\\Downloads\\IEEE_AI_ChatBot\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # Show current working directory\n",
    "print(os.path.isfile('.env'))  # Check if .env exists in this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db016d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env loaded: True\n",
      "Current working directory: c:\\Users\\amrha\\Downloads\\IEEE_AI_ChatBot\n",
      ".env exists: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "result = load_dotenv()  # take environment variables from .env.\n",
    "print(f\".env loaded: {result}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\".env exists: {os.path.isfile('.env')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "884edda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff4a7fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "885b21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fa73d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_api_key = PINECONE_API_KEY\n",
    "pc=Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e328e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.pinecone.Pinecone at 0x18a4da01fd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79150879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "index_name = \"ieee-ai-chat\"  # Only lowercase, numbers, and hyphens\n",
    "if index_name not in pc.list_indexes():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # Dimension of the embedding model\n",
    "        metric=\"cosine\",  # Similarity metric\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87f709b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29d607cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "docsearch = PineconeVectorStore.from_documents(documents=texts_chunk,embedding=embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef233b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing index\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "docsearch = PineconeVectorStore.from_existing_index(embedding=embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96a0018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ADD More Data to the existing index\n",
    "# dswish=Document(page_content=\"Diabetes is a chronic condition that affects how your body turns food into energy. Most of the food you eat is broken down into sugar (also called glucose) and released into your bloodstream. When your blood sugar goes up, it signals your pancreas to release insulin. Insulin acts like a key to let the blood sugar into your body's cells for use as energy.\",\n",
    "#                  metadata={\"source\": \"diabetes.txt\"})\n",
    "# docsearch.add_documents(documents=[dswish])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04d0f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3e7ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_docs=retriever.get_relevant_documents(\"What is AI Committee?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f11e5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='5db1457b-db63-413c-a0bb-3f295b383e00', metadata={'source': 'data\\\\IEEE Beni Suef Student Branch.pdf'}, page_content=\"- **AI Committee:** \\n \\n        ### **Introduction** \\n \\n        **A Journey of Learning, Innovation, and Teamwork** \\n \\n        In today's fast-paced digital world, artificial intelligence skills are no longer optional; they are \\nessential. That's why the IEEE Artificial Intelligence Committee became the perfect place to start a \\njourney of growth. From learning the basics of machine learning and data handling, we moved toward\"),\n",
       " Document(id='6aa8b330-95c1-4544-b9fa-9eff7698e21b', metadata={'source': 'data\\\\IEEE Beni Suef Student Branch.pdf'}, page_content=\"# IEEE Beni Suef Student Branch Website Articles \\n \\n- **Technical Committees** \\n \\n    The Technical Committees are the scientific engine of IEEE Beni-Suef. They focus on developing \\nstudents' skills in **computer science, engineering, and space fields**, turning theoretical knowledge \\ninto practical projects in AI, Cybersecurity, Electronics, Web Development, Data Science, and more. \\n \\n    - **Astronomy Committee:** \\n \\n        ### **Introduction**\"),\n",
       " Document(id='62c168a0-fa41-47e7-9287-ca62f7024d30', metadata={'source': 'data\\\\IEEE Beni Suef Student Branch.pdf'}, page_content=\"The **IEEE Artificial Intelligence Committee** is more than a learning space; it's a community \\nwhere passion meets practice. If you're looking to grow in **machine learning**, from **basics** to \\n**advanced projects**, this is your chance to start. \\n \\n        Join the IEEE Community today and make your mark in the world of **artificial intelligence** - dive \\nin now and contribute to innovative solutions that shape tomorrow! \\n \\n    - **Cybersecurity Committee:**\")]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5ab4213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9898c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# chatModel = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0bb1770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acromegaly and gigantism are both caused by the same underlying problem: **excess growth hormone (GH) in the body**.  The difference lies primarily in *when* the excess GH occurs:\n",
      "\n",
      "* **Gigantism:** This condition develops *before* puberty, when the growth plates in the bones are still open.  The excess GH causes excessive growth in height, resulting in individuals who are abnormally tall.  Other features may include enlarged organs and other features related to excess GH.\n",
      "\n",
      "* **Acromegaly:** This condition develops *after* puberty, when the growth plates have closed.  Because the bones can no longer lengthen, the excess GH instead causes an enlargement of the bones in the hands, feet, and face.  This leads to characteristic features like:\n",
      "    * Enlarged hands and feet (with thickened skin)\n",
      "    * Protruding jaw (prognathism)\n",
      "    * Thickened facial features\n",
      "    * Enlarged nose and lips\n",
      "    * Deepened voice\n",
      "    * Headaches\n",
      "    * Joint pain\n",
      "    * Sleep apnea\n",
      "    * Organ enlargement (heart, liver, etc.)\n",
      "    * Increased sweating and body odor\n",
      "    * Impaired glucose tolerance (leading to diabetes)\n",
      "\n",
      "In both conditions, the excess GH is usually due to a benign tumor (adenoma) on the pituitary gland, which is responsible for producing GH.  Less commonly, it can be caused by other factors.  Treatment involves managing the tumor (often surgically) and controlling the excess GH levels, typically with medication.  Early diagnosis and treatment are important to minimize long-term complications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# ضيف الـ API key\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# اختار الموديل (فيه اختيارات زي gemini-1.5-flash أو gemini-1.5-pro)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# جرب تسأل سؤال\n",
    "response = model.generate_content(\"What is Acromegaly and gigantism?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "468b0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "chatModel = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0aa454cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "\"You are \\\"IEEE_AI_ChatBot\\\", the official assistant for IEEE Beni-Suef Student Branch. Your goal is to help students and members with accurate, polite, and actionable answers about IEEE activities, committees, learning resources, events, technical guidance, and how to join or contribute.\"\n",
    "\"Knowledge base (use when answering):\"\n",
    "\"Technical Committees: Astronomy, Power, Embedded Systems, GNC, Frontend, Backend, AI, Cybersecurity, Data Analysis. These committees focus on turning theoretical knowledge into practical projects across computer science, engineering, and space fields.\"\n",
    "\"Non-Technical Committees: Social Media, Graphic Design, HR, PR, Logistics, Entrepreneurship. These committees handle organization, creativity, communications, and event delivery.\"\n",
    "\"Behavior & tone:\"\n",
    "\"Default language: English. Match the user's language if they explicitly use another language.\"\n",
    "\"Be concise, friendly, and professional. Start with a short direct answer for quick questions, then offer an expanded explanation or next steps if needed.\"\n",
    "\"If the user asks for technical help (code, configs, architecture), provide runnable examples, clearly state assumptions, and use `df` for dataset variables unless the user requests a different name.\"\n",
    "\"For requests about events, joining, or organization, provide checklists, roles, timelines, and templates (e.g., meeting agenda, onboarding checklist).\"\n",
    "\"Clarification policy:\"\n",
    "\"If the users question is ambiguous, ask **one** short clarifying question, then proceed.\"\n",
    "\"Fallback & web search:\"\n",
    "\"If you cannot confidently answer from your knowledge, perform a web search for up-to-date information. After searching, present a concise summary and prepend it with:\"\n",
    "\"\\\"I searched for this and found the following:\\\"\"\n",
    "\"Then list findings and include source links or citations when available.\"\n",
    "\"If web access is unavailable, say: \\\"I don't have web access right now; here's what I can suggest based on common practice,\\\" and provide practical alternatives (internal IEEE contacts, how to verify, or typical steps).\"\n",
    "\"Safety & quality:\"\n",
    "\"Never invent facts. If unsure, say \\\"I don't know\\\" and offer ways to verify.\"\n",
    "\"Avoid sensitive personal data and partisan or medical/legal advice.\"\n",
    "\"Prioritize clarity and actionable guidance (steps, templates, examples).\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e52e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6401c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "chatModel = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f39b33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93bc8f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IEEE Beni-Suef Student Branch AI Committee is a community for students interested in learning and applying artificial intelligence.  It provides opportunities to learn machine learning, from basic concepts to advanced projects, and contribute to innovative AI solutions.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is AI Committee?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "12004cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    {\"question\": \"What is AI Committee?\", \"answer\": \"The IEEE Beni-Suef Student Branch AI Committee is a community for students interested in learning and applying artificial intelligence.  It provides opportunities to learn machine learning, from basic concepts to advanced projects, and contribute to innovative AI solutions.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c6ba73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for item in test_questions:\n",
    "    response = rag_chain.invoke({\"input\": item[\"question\"]})\n",
    "    results.append({\"question\": item[\"question\"], \"model_answer\": response[\"answer\"], \"true_answer\": item[\"answer\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "569b37cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score: 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "def f1_score(pred, true):\n",
    "    pred_tokens = set(pred.split())\n",
    "    true_tokens = set(true.split())\n",
    "    common = pred_tokens & true_tokens\n",
    "    if not common:\n",
    "        return 0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(true_tokens)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "scores = [f1_score(r[\"model_answer\"], r[\"true_answer\"]) for r in results]\n",
    "average_f1 = sum(scores) / len(scores)\n",
    "print(\"Average F1 Score:\", average_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
